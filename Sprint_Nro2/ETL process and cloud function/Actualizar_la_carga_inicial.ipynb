{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credenciales configuradas desde: data-avatar-435301-p6-45ed205c0371.json\n",
      "Procesando el archivo: metadata_filtrada_final.parquet\n",
      "Columna 'category' transformada a una cadena de texto separada por comas.\n",
      "Columna 'state' agregada al archivo 'metadata_filtrada_final.parquet'.\n",
      "Archivo subido exitosamente.\n",
      "Archivo metadata_filtrada_final.parquet actualizado y subido correctamente.\n",
      "Procesando el archivo: reviews_negocios_con_texto_filtrados.parquet\n",
      "Archivo subido exitosamente.\n",
      "Archivo reviews_negocios_con_texto_filtrados.parquet actualizado y subido correctamente.\n",
      "Procesando el archivo: reviews_negocios_sin_texto_filtrados.parquet\n",
      "Archivo subido exitosamente.\n",
      "Archivo reviews_negocios_sin_texto_filtrados.parquet actualizado y subido correctamente.\n",
      "Todos los archivos han sido procesados y actualizados.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tempfile  # Para manejar carpetas temporales de forma multiplataforma\n",
    "\n",
    "# Configurar las credenciales de Google Cloud\n",
    "def configure_google_cloud_credentials(credential_path):\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credential_path\n",
    "    print(f\"Credenciales configuradas desde: {credential_path}\")\n",
    "\n",
    "# Configurar las credenciales de Google Cloud (Cambiar el path a la ubicación de tu archivo JSON de credenciales)\n",
    "configure_google_cloud_credentials(\"data-avatar-435301-p6-45ed205c0371.json\")\n",
    "\n",
    "# Función para transformar la columna 'category'\n",
    "def transform_category_column(df):\n",
    "    if 'category' in df.columns:\n",
    "        # Convertir 'category' en una cadena de texto separada por comas\n",
    "        df['category'] = df['category'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "        print(\"Columna 'category' transformada a una cadena de texto separada por comas.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función para asignar la abreviatura del estado basado en la latitud y longitud\n",
    "def assign_state(row):\n",
    "    lat, lon = row['latitude'], row['longitude']\n",
    "    \n",
    "    # California\n",
    "    if 32.5 <= lat <= 42 and -124.4 <= lon <= -114.1:\n",
    "        return 'CA'  # California\n",
    "    # Florida\n",
    "    elif 24.5 <= lat <= 31 and -87.6 <= lon <= -80.0:\n",
    "        return 'FL'  # Florida\n",
    "    # Illinois\n",
    "    elif 36.9 <= lat <= 42.5 and -91.5 <= lon <= -87.0:\n",
    "        return 'IL'  # Illinois\n",
    "    # Nueva York\n",
    "    elif 40.5 <= lat <= 45 and -79.8 <= lon <= -71.8:\n",
    "        return 'NY'  # Nueva York\n",
    "    else:\n",
    "        return 'Unknown'  # Si la latitud/longitud no coincide con ningún estado\n",
    "\n",
    "# Función para subir archivos al bucket con manejo de tiempo de espera\n",
    "def upload_to_bucket(blob, local_file_path):\n",
    "    try:\n",
    "        with open(local_file_path, \"rb\") as f:\n",
    "            # Aumenta el tiempo de espera\n",
    "            blob.upload_from_file(f, timeout=600)  # 600 segundos (10 minutos)\n",
    "            print(\"Archivo subido exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error subiendo el archivo: {str(e)}\")\n",
    "\n",
    "# Función para actualizar la carga inicial en todos los archivos del bucket 'data limpia'\n",
    "def update_all_files_in_bucket(bucket_name='g1_datos_limpios'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Listar todos los archivos en el bucket\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Verificar si el archivo es un archivo Parquet\n",
    "        if blob.name.endswith('.parquet'):\n",
    "            print(f\"Procesando el archivo: {blob.name}\")\n",
    "\n",
    "            # Leer el archivo Parquet\n",
    "            existing_data = read_parquet_from_gcs(bucket_name, blob.name)\n",
    "\n",
    "            if existing_data is not None:\n",
    "                # Verificar y agregar columnas 'version' y 'fecha_ingreso' si no existen\n",
    "                if 'version' not in existing_data.columns:\n",
    "                    existing_data['version'] = 1  # Asignar la versión 1 para la carga inicial\n",
    "                if 'fecha_ingreso' not in existing_data.columns:\n",
    "                    existing_data['fecha_ingreso'] = datetime.now()  # Asignar la fecha y hora actual\n",
    "\n",
    "                # Aplicar la transformación de la columna 'category' y 'assign_state' solo si es el archivo 'metadata_filtrada_final.parquet'\n",
    "                if blob.name == \"metadata_filtrada_final.parquet\":\n",
    "                    existing_data = transform_category_column(existing_data)\n",
    "                    if 'latitude' in existing_data.columns and 'longitude' in existing_data.columns:\n",
    "                        # Aplicar la función assign_state para agregar la columna 'state'\n",
    "                        existing_data['state'] = existing_data.apply(assign_state, axis=1)\n",
    "                        print(\"Columna 'state' agregada al archivo 'metadata_filtrada_final.parquet'.\")\n",
    "\n",
    "                # Guardar el archivo actualizado temporalmente en una ruta temporal válida\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".parquet\") as temp_file:\n",
    "                    local_file_path = temp_file.name\n",
    "                    existing_data.to_parquet(local_file_path)\n",
    "                    \n",
    "                    # Subir el archivo actualizado de nuevo al bucket\n",
    "                    updated_blob = bucket.blob(blob.name)\n",
    "                    upload_to_bucket(updated_blob, local_file_path)\n",
    "                \n",
    "                # Eliminar el archivo temporal\n",
    "                os.remove(local_file_path)\n",
    "                print(f\"Archivo {blob.name} actualizado y subido correctamente.\")\n",
    "            else:\n",
    "                print(f\"No se pudo procesar el archivo: {blob.name}\")\n",
    "    \n",
    "    print(\"Todos los archivos han sido procesados y actualizados.\")\n",
    "\n",
    "# Función para leer archivos Parquet desde GCS\n",
    "def read_parquet_from_gcs(bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    try:\n",
    "        with blob.open(\"rb\") as f:\n",
    "            df = pd.read_parquet(f)\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo el archivo {blob_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar la función para actualizar todos los archivos\n",
    "update_all_files_in_bucket()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
